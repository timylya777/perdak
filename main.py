import os
import subprocess
import time
from fastapi import FastAPI, Request, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import RedirectResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from urllib.parse import urlparse
import secrets
import sqlite3
from contextlib import contextmanager
from datetime import datetime
import uvicorn
import ollama

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
env = os.environ.copy()
env['OLLAMA_GPU'] = 'cuda'  # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU

app = FastAPI(title="NeuroChat API", version="1.0")

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FRONTEND_DIR = os.path.join(BASE_DIR, "frontend")
DB_PATH = os.path.join(BASE_DIR, "links.db")

# –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory=os.path.join(FRONTEND_DIR, "static")), name="static")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–æ–≤
templates = Jinja2Templates(directory=os.path.join(FRONTEND_DIR, "templates"))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
OLLAMA_MODEL = "llama3:8b-instruct-q4_0"
OLLAMA_HOST = "127.0.0.1:11434"

def setup_ollama_config():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Ollama –¥–ª—è GPU"""
    config_dir = os.path.expanduser("~/.ollama")
    config_file = os.path.join(config_dir, "config.json")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(config_dir, exist_ok=True)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è GPU
    config = {
        "host": OLLAMA_HOST,
        "num_gpu": 1,           # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
        "num_thread": 4,        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
        "batch_size": 512,      # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        "main_gpu": 0,          # –û—Å–Ω–æ–≤–Ω–∞—è –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞
    }
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(config_file, 'w') as f:
        import json
        json.dump(config, f, indent=2)
    
    print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Ollama —Å–æ–∑–¥–∞–Ω–∞: {config_file}")

def check_ollama_status():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        ollama.list()
        print("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")
        return True
    except Exception as e:
        print(f"‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return False

def start_ollama_server():
    """–ó–∞–ø—É—Å–∫ Ollama —Å–µ—Ä–≤–µ—Ä–∞ –µ—Å–ª–∏ –Ω–µ –∑–∞–ø—É—â–µ–Ω"""
    if check_ollama_status():
        return True
    
    print("üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º Ollama —Å–µ—Ä–≤–µ—Ä...")
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º Ollama –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
        subprocess.Popen(["ollama", "serve"], 
                        stdout=subprocess.DEVNULL, 
                        stderr=subprocess.DEVNULL,
                        env=env)  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
        
        # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞
        for _ in range(10):
            time.sleep(1)
            if check_ollama_status():
                print("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
                return True
        
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä")
        return False
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Ollama: {e}")
        return False

def check_model_availability():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
    try:
        models = ollama.list()
        model_names = [model['name'] for model in models['models']]
        
        if OLLAMA_MODEL in model_names:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –¥–æ—Å—Ç—É–ø–Ω–∞")
            return True
        else:
            print(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {model_names}")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        return False

def download_model_if_needed():
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"""
    if check_model_availability():
        return True
    
    print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å {OLLAMA_MODEL}...")
    try:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏
        stream = ollama.pull(OLLAMA_MODEL, stream=True)
        for progress in stream:
            if 'completed' in progress and 'total' in progress:
                percent = (progress['completed'] / progress['total']) * 100
                print(f"–ü—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏: {percent:.1f}%")
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

@app.on_event("startup")
async def startup_event():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ NeuroChat API...")
    print("üéÆ –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ (NVIDIA CUDA)")
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Ollama
    setup_ollama_config()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º Ollama —Å–µ—Ä–≤–µ—Ä
    if not start_ollama_server():
        print("‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ Ollama —Å–µ—Ä–≤–µ—Ä–∞")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if not check_model_availability():
        print("üîÑ –ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å...")
        download_model_if_needed()
    
    print("‚úÖ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ —Ä–∞–±–æ—Ç–µ")

@app.get("/")
async def read_root(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.post("/api/chat")
async def api_chat(request: Request):
    """Endpoint –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º"""
    try:
        data = await request.json()
        question = data.get("message", data.get("question", "")).strip()
        
        if not question:
            return JSONResponse({"error": "–°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"}, status_code=400)
        
        print(f"üí¨ –ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ /api/chat: {question[:50]}...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É —á—Ç–æ –∏ –≤ /ai-question
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{"role": "user", "content": question}],
            options={
                "num_gpu": 1,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU
                "num_thread": 8,
                "num_predict": 384,
                "temperature": 0.7,
                "top_p": 0.9,
            }
        )
        
        ai_response = response['message']['content']
        
        return {
            "message": ai_response,
            "response": ai_response,  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            "question": question,
            "model": OLLAMA_MODEL
        }
        
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        ollama_status = check_ollama_status()
        model_status = check_model_availability()
        
        return {
            "status": "healthy" if ollama_status and model_status else "degraded",
            "ollama": "running" if ollama_status else "not running",
            "model": "available" if model_status else "not available",
            "model_name": OLLAMA_MODEL,
            "gpu": "enabled"  # –í—Å–µ–≥–¥–∞ enabled, —Ç–∞–∫ –∫–∞–∫ –º—ã –Ω–∞—Å—Ç—Ä–æ–∏–ª–∏ GPU
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}, 500

@app.post("/ai-question")
async def get_ai_response(request: Request):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ AI"""
    try:
        data = await request.json()
        question = data.get("question", "").strip()
        
        if not question:
            return JSONResponse({"error": "–í–æ–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"}, status_code=400)
        
        if len(question) > 1000:
            return JSONResponse({"error": "–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å"}, status_code=400)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        if not check_ollama_status():
            return JSONResponse({
                "error": "Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve"
            }, status_code=503)
        
        print(f"ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {question[:50]}...")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU
        start_time = time.time()
        
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[{"role": "user", "content": question}],
            options={
                "num_gpu": 1,  # –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU
                "num_thread": 8,
                "num_predict": 512,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
            }
        )
        
        processing_time = time.time() - start_time
        ai_response = response['message']['content']
        
        print(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {processing_time:.2f} —Å–µ–∫, –¥–ª–∏–Ω–∞: {len(ai_response)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return {
            "question": question,
            "answer": ai_response,
            "processing_time": round(processing_time, 2),
            "model": OLLAMA_MODEL
        }
        
    except ConnectionError:
        return JSONResponse({
            "error": "–°–µ—Ä–≤–µ—Ä AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–ø—É—â–µ–Ω –ª–∏ Ollama: ollama serve"
        }, status_code=503)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return JSONResponse({
            "error": f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}"
        }, status_code=500)

@app.get("/models")
async def get_available_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        models = ollama.list()
        return {"models": models['models']}
    except Exception as e:
        return {"error": str(e)}, 500

@app.get("/system-info")
async def get_system_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        model_info = ollama.show(OLLAMA_MODEL)
        
        return {
            "gpu_available": True,  # –¢–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ true
            "gpu_enabled": True,    # –î–æ–±–∞–≤–ª—è–µ–º —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ
            "model": OLLAMA_MODEL,
            "model_parameters": model_info.get('parameters', 'unknown'),
            "model_size": model_info.get('size', 'unknown'),
            "system": "Windows" if os.name == 'nt' else "Linux/Mac",
            "gpu_type": "NVIDIA CUDA"
        }
    except Exception as e:
        return {"gpu_available": False, "error": str(e)}

# –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ë–î
@contextmanager
def get_db_connection():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
def init_db():
    with get_db_connection() as conn:
        conn.execute('''
            CREATE TABLE IF NOT EXISTS chat_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT NOT NULL,
                answer TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                model TEXT DEFAULT 'llama3'
            )
        ''')
        conn.commit()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ë–î –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
init_db()

if __name__ == "__main__":
    print("üåü –ó–∞–ø—É—Å–∫ NeuroChat —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –Ω–∞ RTX 4060")
    print("üéÆ GPU –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω: NVIDIA CUDA")
    print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å: {OLLAMA_MODEL}")
    print("üåê –°–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://127.0.0.1:25567")
    
    uvicorn.run(
        app, 
        host="127.0.0.1", 
        port=25567,
        log_level="info",
        timeout_keep_alive=300  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    )